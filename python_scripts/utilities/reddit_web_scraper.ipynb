{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5fGZ-rJhfEbL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "#import tracemalloc\n",
        "#tracemalloc.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 39 (807189367.py, line 40)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 40\u001b[1;36m\u001b[0m\n\u001b[1;33m    script = \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 39\n"
          ]
        }
      ],
      "source": [
        "class Post(object):\n",
        "    def __init__(self, url=\"\"):\n",
        "        self.url = url\n",
        "        self.date = \"\"\n",
        "        self.title = \"\"\n",
        "        self.content = \"\"\n",
        "        self.comments = []\n",
        "        self.weeks_ago = -1\n",
        "        self.subreddit = \"\"\n",
        "        self.author = \"\"  # unused\n",
        "\n",
        "    def to_dict(self):\n",
        "        post_dict = {\n",
        "            \"url\": self.url,\n",
        "            \"date\": self.date,\n",
        "            \"title\": self.title,\n",
        "            \"content\": self.content,\n",
        "            \"comments\": self.comments,\n",
        "            \"author\": self.author,\n",
        "            \"weeks_ago\": self.weeks_ago,\n",
        "            \"subreddit\": self.subreddit\n",
        "        }\n",
        "        return post_dict\n",
        "\n",
        "def load_post(obj):\n",
        "    post = Post(obj[\"url\"])\n",
        "    post.date = obj[\"date\"]\n",
        "    post.title = obj[\"title\"]\n",
        "    post.content = obj[\"content\"]\n",
        "    post.comments = obj[\"comments\"]\n",
        "    post.author = obj[\"author\"]\n",
        "    post.weeks_ago = obj[\"weeks_ago\"]\n",
        "    post.subreddit = obj[\"subreddit\"]\n",
        "    return post\n",
        "\n",
        "\n",
        "class Searcher(object):\n",
        "\n",
        "    def __init__(self, username, password):\n",
        "        self.driver = webdriver.Firefox()\n",
        "        self.subreddit_posts = {}\n",
        "        self.vis = set({})\n",
        "        self.finished = set({})\n",
        "        self.initialize(username, password)\n",
        "\n",
        "    def initialize(self, username, password):\n",
        "        loginURL = \"https://www.reddit.com/login/?dest=https%3A%2F%2Fwww.reddit.com%2F\"\n",
        "        self.driver.get(loginURL)\n",
        "        self.get_waited(\"//input[@id='login-username']\").send_keys(username)\n",
        "        self.get_waited(\"//input[@id='login-password']\").send_keys(password)\n",
        "        self.get_waited(\"//button[@class='AnimatedForm__submitButton m-full-width']\").click()\n",
        "        time.sleep(4)\n",
        "        return\n",
        "\n",
        "    def get_waited(self, xpath):\n",
        "        return WebDriverWait(self.driver, 10).until(EC.visibility_of_element_located((By.XPATH, xpath)))\n",
        "\n",
        "    def filter_tags(self, s):\n",
        "        inTag = False\n",
        "        currString = \"\"\n",
        "        response = []\n",
        "        for letter in s:\n",
        "            if letter == \"<\":\n",
        "                inTag = True\n",
        "                if currString:\n",
        "                    response.append(currString)\n",
        "                currString = \"\"\n",
        "            elif letter == \">\":\n",
        "                inTag = False\n",
        "            else:\n",
        "                if not inTag:\n",
        "                    currString += letter\n",
        "        if currString:\n",
        "            response += currString\n",
        "\n",
        "        return \"\".join(response)\n",
        "\n",
        "    def get_nontags(self, s):\n",
        "        return re.sub(\"<.+?>\", \"\", s).strip()\n",
        "\n",
        "    def get_texts(self, property, args={}):\n",
        "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "        if args:\n",
        "            tags = soup.find_all(property, args)\n",
        "        else:\n",
        "            tags = soup.find_all(property)\n",
        "        texts = [self.get_nontags(str(tag)) for tag in tags]\n",
        "        return texts\n",
        "\n",
        "\n",
        "    def extract_property_values(self, property, tags):\n",
        "        property_values = []\n",
        "        for tag in tags:\n",
        "            if type(tag) != str:\n",
        "                tag = str(tag)\n",
        "            lst = tag.split(property+\"=\")\n",
        "            if len(lst) == 1:\n",
        "                continue\n",
        "            value = \"\".join(lst[1:])\n",
        "            value = value[1:]\n",
        "            value = value[:value.index(\"\\\"\")]\n",
        "            property_values.append(value)\n",
        "        return property_values\n",
        "\n",
        "\n",
        "    def filter_by_contains_property(self, property_names, tags):\n",
        "        filtered_tags = []\n",
        "        for tag in tags:\n",
        "            if type(tag) != str:\n",
        "                tag = str(tag)\n",
        "            matching_properties = 0\n",
        "            property_values = tag.split(\" \")\n",
        "            for property_value in property_values:\n",
        "                if \"=\" not in property_value:\n",
        "                    continue\n",
        "                if property_value.split(\"=\")[0] in property_names:\n",
        "                    matching_properties += 1\n",
        "            if matching_properties == len(property_names):\n",
        "                filtered_tags.append(tag)\n",
        "        return filtered_tags\n",
        "\n",
        "    def get_similar_questions(self, soup):\n",
        "        similar_questions = soup.find_all(\"span\", {\"class\": \"CSkcDe\"})\n",
        "        similar_questions = list(map(str, similar_questions))\n",
        "        similar_questions = list(map(self.filter_tags, similar_questions))\n",
        "        return similar_questions\n",
        "\n",
        "    def get_answers(self, soup):\n",
        "        answers = soup.find_all(\"span\", {\"class\": \"hgKElc\"})\n",
        "        answers = list(map(str, answers))\n",
        "        answers = list(map(self.filter_tags, answers))\n",
        "        return answers\n",
        "\n",
        "    def fill_post(self, post, postURL=\"\"):\n",
        "        if not postURL:\n",
        "            postURL = post.url\n",
        "        print(\"filling post:\", postURL)\n",
        "\n",
        "        self.driver.get(postURL)\n",
        "        date_item = self.get_waited(\"//span[@data-testid='post_timestamp']\").text\n",
        "        date_item = date_item.strip()\n",
        "        print(f\" {str(date_item)=} \")\n",
        "\n",
        "        current_date = datetime.now().date()\n",
        "        # Subtract x days\n",
        "        if \"hours\" in date_item:\n",
        "            days_ago_str = \"0\"\n",
        "        elif \"days\" in date_item:\n",
        "            days_ago_str = date_item[:date_item.index(\" \")]\n",
        "        elif \"weeks\" in date_item:\n",
        "            days_ago_str = str(int(date_item[:date_item.index(\" \")]) * 7)\n",
        "        elif \"months\" in date_item:\n",
        "            days_ago_str = str(int(date_item[:date_item.index(\" \")]) * 30)\n",
        "        else:\n",
        "            days_ago_str = None\n",
        "\n",
        "        if days_ago_str:\n",
        "            x = int(days_ago_str) # Number of days to subtract\n",
        "            post.weeks_ago = x // 7\n",
        "            post_date = current_date - timedelta(days=x)\n",
        "            year = str(post_date.year)\n",
        "            month = str(post_date.month).zfill(2)  # zfill adds leading zeros if necessary\n",
        "            day = str(post_date.day).zfill(2)\n",
        "            post_date_string = \"-\".join([year, month, day])\n",
        "            post.date = post_date_string\n",
        "            print(f\"{post.date=} {post.weeks_ago}\\n\")\n",
        "\n",
        "        # Note, returns a list but will be length 1\n",
        "        titles = self.get_texts(\"h1\")\n",
        "        if titles:\n",
        "            post.title = titles[0]\n",
        "\n",
        "        # Load comments\n",
        "        prev = 0\n",
        "        for __load_results__ in range(6):\n",
        "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            # Check if anything new has loaded\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            ps = [str(p) for p in soup.find_all(\"p\")]\n",
        "            if len(ps) == prev:\n",
        "                break\n",
        "            else:\n",
        "                prev = len(ps)\n",
        "\n",
        "        ps = self.get_texts(\"p\")\n",
        "        if not ps:\n",
        "            print(\"no <p> tags found!\", postURL)\n",
        "            return post\n",
        "        if ps[0].strip() == \"no comments yet\":\n",
        "            return ps\n",
        "\n",
        "        for i in range(1, len(ps)):\n",
        "            post.comments.append(ps[i])\n",
        "\n",
        "        return post\n",
        "\n",
        "\n",
        "    def get_subreddit_posts(self, subreddit, subredditURL):\n",
        "        if subreddit in self.finished:\n",
        "            return []\n",
        "\n",
        "        # Get the subreddit's post's links\n",
        "        subreddit_post_links = []\n",
        "\n",
        "        # Load the posts.\n",
        "        for __load_results__ in range(6000):\n",
        "            # Wait for the page to load and the new search results to appear\n",
        "            time.sleep(1)\n",
        "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "        time.sleep(10)\n",
        "\n",
        "\n",
        "\n",
        "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
        "        anchor_tags = soup.find_all('a')\n",
        "        anchor_tags = self.filter_by_contains_property([\"href\"], anchor_tags)\n",
        "        subreddit_post_links += self.extract_property_values(\"href\", anchor_tags)\n",
        "\n",
        "        # Filter the links\n",
        "        target = \"/r/\"+subreddit+\"/comments/\"\n",
        "        postURLs = [url for url in subreddit_post_links if target.lower() in url.lower()]\n",
        "        postURLs = [url for url in postURLs]\n",
        "        for i in range(len(postURLs)):\n",
        "            if \"www.reddit.com\" not in postURLs[i]:\n",
        "                postURLs[i] = \"https://www.reddit.com\" + postURLs[i]\n",
        "\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(len(subreddit_post_links))\n",
        "        print(f\"{subreddit_post_links[0]=}\\n\")\n",
        "        print(f\"{subreddit_post_links[-1]=}\\n\")\n",
        "\n",
        "        posts = []\n",
        "        for postURL in postURLs:\n",
        "            if postURL not in self.vis:\n",
        "                self.vis.add(postURL)\n",
        "            else:\n",
        "                print(\"skipping\", postURL)\n",
        "                continue\n",
        "            post = Post(postURL)\n",
        "            # Collect data for the post\n",
        "            try:\n",
        "                self.fill_post(post)\n",
        "                if post.weeks_ago >= 52:\n",
        "                    self.finished.add(subreddit)\n",
        "                post.subreddit = subreddit\n",
        "                posts.append(post)\n",
        "\n",
        "                # Save the data (I'm using a local cache)\n",
        "                with open(\"posts.txt\", \"a\") as f:\n",
        "                    obj = post.to_dict()\n",
        "                    s = json.dumps(obj)\n",
        "                    f.write(s+\"\\n\")\n",
        "            # Due to the request not responding\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "        # Optionally Memoize it\n",
        "        self.subreddit_posts[subreddit] = posts\n",
        "        return posts\n",
        "\n",
        "####\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def get_posts(subreddit, searcher):\n",
        "    subredditURL = \"https://www.reddit.com/r/\" + subreddit + \"/new/\"\n",
        "    searcher.driver.get(subredditURL)\n",
        "    posts = searcher.get_subreddit_posts(subreddit, subredditURL)\n",
        "    return posts\n",
        "\n",
        "\n",
        "def collect(subreddits, searcher):\n",
        "    posts = []\n",
        "    for subreddit in subreddits:\n",
        "        posts += get_posts(subreddit, searcher)\n",
        "    return posts\n",
        "\n",
        "\n",
        "####\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "def main(username, passwords, subreddits):\n",
        "    print(\"----------------------\")\n",
        "    print(\"main() function called\")\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    with open(\"filtered-posts.txt\", \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        lines = [l.strip() for l in lines]\n",
        "        objs = [json.loads(line) for line in lines]\n",
        "        # can not set obj here. Will all be pointer to the same variable\n",
        "        posts = [load_post(obj) for obj in objs]\n",
        "    \"\"\"\n",
        "\n",
        "    # This is an account I made for this task\n",
        "\n",
        "    searcher = Searcher(username, password)\n",
        "\n",
        "\n",
        "\n",
        "    posts = collect(subreddits, searcher)\n",
        "\n",
        "    return\n",
        "\n",
        "username = \"sagecsgocollector\"\n",
        "password = \" x*P9v:e4mRVqB\"\n",
        "subreddits = [\"csgo\"] #can add more subreddits\n",
        "main(username, password, subreddits)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
