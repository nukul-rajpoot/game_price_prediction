{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from kerastuner import RandomSearch, Hyperband, BayesianOptimization\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# print(cwd)\n",
    "\n",
    "# # for windownoobs\n",
    "GAME_PRICE_PREDICTION_PATH = os.path.abspath(os.path.join(cwd, '..', '..'))\n",
    "\n",
    "# # for mac\n",
    "#GAME_PRICE_PREDICTION_PATH = os.path.abspath(os.path.join(cwd))\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(GAME_PRICE_PREDICTION_PATH))\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    # Replace disallowed characters with underscores\n",
    "    filename = re.sub(r'[\\\\/*?:\"<>|]', '_', filename)\n",
    "    # Replace spaces with underscores\n",
    "    filename = re.sub(r'\\s+', '_', filename)\n",
    "    # Convert filename to lowercase\n",
    "    filename = filename.lower()\n",
    "    return filename\n",
    "\n",
    "# input the skibbity item's name here :P  NOTE: ITEMS = [] for mispelled items\n",
    "ITEM = \"M4A1-S | Golden Coil (Factory New)\"\n",
    "ITEM_SANITIZED = sanitize_filename(ITEM)\n",
    "\n",
    "# For filter_file               |    NOTE: (input: compressed -> output: filtered_data (MS) )\n",
    "INPUT_COMPRESSED = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'reddit_data', 'compressed_data')\n",
    "FILTERED_DATA_DIRECTORY = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'reddit_data', 'filtered_data', f'{ITEM_SANITIZED}_filtered')\n",
    "\n",
    "# for mention_counter           |   NOTE:  (input: filtered_data (MS) -> output: mention_data (MS))\n",
    "MENTION_DATA_DIRECTORY = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'reddit_data', 'mention_data', f'{ITEM_SANITIZED}_mentions')\n",
    "\n",
    "# mention_data_combiner         |   NOTE:  (input: mention_data (MS) -> output: mention_ALL)  - directories N/A for this.\n",
    "ALL_MENTIONS_FILENAME = f'{ITEM_SANITIZED}_all_mentions.csv'\n",
    "ALL_MENTIONS_DATA = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'reddit_data', 'mention_all', ALL_MENTIONS_FILENAME)\n",
    "\n",
    "# For vader_polarity            |    NOTE: (input: filtered -> output: polarity_data)\n",
    "POLARITY_FOLDER_NAME = f'{ITEM_SANITIZED}_polarity'\n",
    "POLARITY_DATA_DIRECTORY = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'reddit_data', 'polarity_data', POLARITY_FOLDER_NAME)\n",
    "OUTPUT_POLARITY_FORMAT = \"csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get price history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_item_from_api(item, dailyCookie):\n",
    "    # get historical price data of item from API\n",
    "    url = \"https://steamcommunity.com/market/pricehistory/\"\n",
    "    params = {\n",
    "        'country': 'US',\n",
    "        'currency': '1',\n",
    "        'appid': '730',\n",
    "        'market_hash_name': item\n",
    "    }\n",
    "    cookies = {'steamLoginSecure': dailyCookie}\n",
    "\n",
    "    response = requests.get(url, params=params, cookies=cookies)\n",
    "    json_data = response.json()\n",
    "    \n",
    "    # print error message if request failed\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for {item}. Status code: {response.status_code}\")\n",
    "        return None \n",
    "           \n",
    "    # convert and clean data to dataframe object\n",
    "    price_history = json_data['prices']\n",
    "    price_history_df = pd.DataFrame(price_history, columns=['date', 'price_usd', 'volume'])\n",
    "    price_history_df['date'] = pd.to_datetime(price_history_df['date'].str[0:-4], format='%b %d %Y %H')\n",
    "    price_history_df['volume'] = pd.to_numeric(price_history_df['volume'])\n",
    "    price_history_df.set_index('date', inplace=True)\n",
    "   \n",
    "    return price_history_df\n",
    "def fetch_item_to_df(item, dailyCookie):\n",
    "    price_history_df = fetch_item_from_api(item, dailyCookie)\n",
    "    grouped_current_item = price_history_df.groupby(pd.Grouper(freq='D')).agg({\n",
    "    'price_usd':'median',\n",
    "    'volume':'sum'\n",
    "    })\n",
    "    return grouped_current_item\n",
    "\n",
    "def get_cookie_from_blob():\n",
    "    blob_url = \"https://steamgraphsstorage.blob.core.windows.net/container-for-blob/cookie.txt?sp=rwd&st=2024-08-06T20:45:18Z&se=2025-09-10T04:45:18Z&spr=https&sv=2022-11-02&sr=c&sig=MKticGz9P9HPI7iXp1a6yuErc5Sv6P9fY%2FfCbxL0PLg%3D\"\n",
    "    response = requests.get(blob_url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def fetch_items():\n",
    "    items = [\"Glove Case Key\", \"Officer Jacques Beltram | Gendarmerie Nationale\", \"Kilowatt Case\", \"AK-47 | Blue Laminate (Factory New)\", \"Glove Case\", \"★ StatTrak™ Paracord Knife | Case Hardened (Field-Tested)\"]\n",
    "    return items\n",
    "\n",
    "dailyCookie = get_cookie_from_blob()\n",
    "items = fetch_items()\n",
    "current_item = fetch_item_to_df(items[4], dailyCookie)\n",
    "df = current_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mentions data, Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  num_mentions   volume  smoothed_mentions  smoothed_volume\n",
      "1213 2016-12-08          29.0  44747.0               25.7          51431.0\n",
      "1214 2016-12-09          18.0  46648.0               22.7          50727.2\n",
      "1215 2016-12-10          34.0  53017.0               22.8          51092.9\n",
      "1216 2016-12-11          20.0  52132.0               23.4          51439.3\n",
      "1217 2016-12-12          22.0  41960.0               24.1          50187.9\n",
      "...         ...           ...      ...                ...              ...\n",
      "4082 2024-10-16           4.0   3071.0                4.0           3106.2\n",
      "4083 2024-10-17           4.0   2661.0                4.0           3143.2\n",
      "4084 2024-10-18           4.0   3097.0                4.0           3161.4\n",
      "4085 2024-10-19           4.0   3141.0                4.0           3198.9\n",
      "4086 2024-10-20           4.0   3498.0                4.0           3268.5\n",
      "\n",
      "[2874 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80051/892391469.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df = merged_df.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "mentions_df = pd.read_csv(os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'reddit_data', 'mention_all', ALL_MENTIONS_FILENAME))\n",
    "mentions_df['date'] = pd.to_datetime(mentions_df['date'])\n",
    "\n",
    "# Ensure df has a 'date' column\n",
    "if 'date' not in df.columns:\n",
    "    df = df.reset_index()\n",
    "\n",
    "# Merge mentions and volume data\n",
    "merged_df = pd.merge(mentions_df, df[['date', 'volume']], on='date', how='outer').sort_values('date')\n",
    "merged_df = merged_df.fillna(method='ffill')\n",
    "\n",
    "# Smooth mentions and volume using a rolling average\n",
    "merged_df['smoothed_mentions'] = merged_df['num_mentions'].rolling(window=10).mean()\n",
    "merged_df['smoothed_volume'] = merged_df['volume'].rolling(window=10).mean()\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      num_mentions  scaled_mentions   volume  scaled_volume\n",
      "1213          29.0         0.109804  44747.0       0.082008\n",
      "1214          18.0         0.066667  46648.0       0.085654\n",
      "1215          34.0         0.129412  53017.0       0.097872\n",
      "1216          20.0         0.074510  52132.0       0.096174\n",
      "1217          22.0         0.082353  41960.0       0.076662\n"
     ]
    }
   ],
   "source": [
    "# Create scalers\n",
    "mentions_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "volume_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the 'num_mentions' column\n",
    "merged_df['scaled_mentions'] = mentions_scaler.fit_transform(merged_df[['num_mentions']])\n",
    "\n",
    "# Fit and transform the 'volume' column\n",
    "merged_df['scaled_volume'] = volume_scaler.fit_transform(merged_df[['volume']])\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(merged_df[['num_mentions', 'scaled_mentions', 'volume', 'scaled_volume']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df['scaled_mentions'].values.reshape(-1, 1)\n",
    "y = merged_df['scaled_volume'].values\n",
    "\n",
    "# Calculate split indices\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "# Split the data\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 00m 08s]\n",
      "val_loss: 0.0005758625920861959\n",
      "\n",
      "Best val_loss So Far: 0.0005758625920861959\n",
      "Total elapsed time: 00h 08m 00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrajpoot/miniconda3/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 36 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "num_lstm_layers: 3\n",
      "lstm_units_0: 32\n",
      "num_dense_layers: 3\n",
      "dense_units_0: 96\n",
      "dense_activation: relu\n",
      "learning_rate: 0.01\n",
      "lstm_units_1: 96\n",
      "lstm_units_2: 32\n",
      "dense_units_1: 256\n",
      "dense_units_2: 256\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2547e-04 \n",
      "Test Loss: 0.0010431850096210837\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\n",
      "Sample Predictions vs Actual Values:\n",
      "Prediction: 0.04, Actual: 0.01\n",
      "Prediction: 0.04, Actual: 0.01\n",
      "Prediction: 0.04, Actual: 0.01\n",
      "Prediction: 0.04, Actual: 0.01\n",
      "Prediction: 0.04, Actual: 0.01\n",
      "Prediction: 0.04, Actual: 0.01\n",
      "Prediction: 0.04, Actual: 0.00\n",
      "Prediction: 0.04, Actual: 0.00\n",
      "Prediction: 0.04, Actual: 0.01\n",
      "Prediction: 0.04, Actual: 0.01\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Tune the number of LSTM layers and units\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 3)):\n",
    "        model.add(layers.LSTM(\n",
    "            units=hp.Int(f'lstm_units_{i}', min_value=32, max_value=256, step=32),\n",
    "            return_sequences=True if i < hp.Int('num_lstm_layers', 1, 3) - 1 else False,\n",
    "            input_shape=(1, 1) if i == 0 else None\n",
    "        ))\n",
    "    \n",
    "    # Tune the number of dense layers and units\n",
    "    for i in range(hp.Int('num_dense_layers', 1, 3)):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'dense_units_{i}', min_value=32, max_value=256, step=32),\n",
    "            activation=hp.Choice('dense_activation', ['relu', 'tanh'])\n",
    "        ))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # Tune learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='lstm_tuning'\n",
    ")\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, 1))\n",
    "X_val_reshaped = X_val.reshape((X_val.shape[0], 1, 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, 1))\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(X_train_reshaped, y_train, \n",
    "             epochs=50, \n",
    "             validation_data=(X_val_reshaped, y_val),\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for hp in best_hps.values:\n",
    "    print(f\"{hp}: {best_hps.get(hp)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = best_model.evaluate(X_test_reshaped, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.predict(X_test_reshaped)\n",
    "\n",
    "# Print some sample predictions vs actual values\n",
    "print(\"\\nSample Predictions vs Actual Values:\")\n",
    "for i in range(10):\n",
    "    print(f\"Prediction: {predictions[i][0]:.2f}, Actual: {y_test[i]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
