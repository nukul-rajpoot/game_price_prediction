{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Custom imports\n",
    "GAME_PRICE_PREDICTION_PATH = os.environ.get('GAME_PRICE_PREDICTION_PATH', '')\n",
    "sys.path.insert(0, os.path.abspath(GAME_PRICE_PREDICTION_PATH))\n",
    "from python_scripts.utilities.api_calls import get_cookie_from_blob, fetch_item_to_df, fetch_items\n",
    "from python_scripts.sentiment_analysis.config import ITEM, ALL_POLARITY_FILENAME, ITEM_SANITIZED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the correct item is imported from config.py\n",
    "print(f\"ITEM = {ITEM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_file_path = os.path.join(GAME_PRICE_PREDICTION_PATH, 'python_scripts', 'sentiment_analysis', 'filter_file.py')\n",
    "# !python \"{filter_file_path}\"\n",
    "\n",
    "# mention_counter_path = os.path.join(GAME_PRICE_PREDICTION_PATH, 'python_scripts', 'sentiment_analysis', 'mention_counter.py')\n",
    "# !python \"{mention_counter_path}\"\n",
    "\n",
    "# mention_data_combiner_path = os.path.join(GAME_PRICE_PREDICTION_PATH, 'python_scripts', 'sentiment_analysis', 'mention_data_combiner.py')\n",
    "# !python \"{mention_data_combiner_path}\"\n",
    "\n",
    "# vader_polarity_path = os.path.join(GAME_PRICE_PREDICTION_PATH, 'python_scripts', 'sentiment_analysis', 'vader_polarity.py')\n",
    "# !python \"{vader_polarity_path}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch item history and import polarity data for item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_cookie = get_cookie_from_blob()\n",
    "price_volume_df = fetch_item_to_df(ITEM, daily_cookie)\n",
    "\n",
    "polarity_path = os.path.join(\n",
    "        GAME_PRICE_PREDICTION_PATH, \n",
    "        'data', \n",
    "        'reddit_data', \n",
    "        'polarity_all', \n",
    "        ALL_POLARITY_FILENAME\n",
    "    )\n",
    "polarity_df = pd.read_csv(polarity_path)\n",
    "polarity_df['date'] = pd.to_datetime(polarity_df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge dataframes, handle missing values and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure df has a 'date' column\n",
    "if 'date' not in price_volume_df.columns:\n",
    "    price_volume_df = price_volume_df.reset_index()\n",
    "\n",
    "# Merge polarity and price/volume data\n",
    "merged_df = pd.merge(\n",
    "    polarity_df, \n",
    "    price_volume_df[['date', 'price_usd', 'volume']], \n",
    "    on='date', \n",
    "    how='outer'\n",
    ").sort_values('date')\n",
    "\n",
    "# print(\"Missing values before handling:\")\n",
    "# print(merged_df.isnull().sum())\n",
    "\n",
    "# Handle missing values more conservatively:\n",
    "# 1. First handle price/volume data\n",
    "merged_df['price_usd'] = merged_df['price_usd'].ffill(limit=2)  # only fill 2-day gaps\n",
    "merged_df['volume'] = merged_df['volume'].fillna(0)  # no trades = 0 volume\n",
    "\n",
    "# 2. For sentiment data, only interpolate very short gaps\n",
    "sentiment_cols = ['compound', 'pos', 'neu', 'neg']\n",
    "for col in sentiment_cols:\n",
    "    merged_df[col] = merged_df[col].interpolate(method='linear', limit=1)  # only fill 1-day gaps\n",
    "\n",
    "# 3. Drop remaining rows with missing values\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# print(\"\\nMissing values after handling:\")\n",
    "# print(merged_df.isnull().sum())\n",
    "# print(f\"\\nRows remaining: {len(merged_df)}\")\n",
    "\n",
    "# Remove outliers (optional)\n",
    "# z_scores = np.abs(stats.zscore(merged_df[['compound', 'price_usd', 'volume']]))\n",
    "# merged_df = merged_df[(z_scores < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timeseries - compound polarity against volume plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for this analysis\n",
    "polarity_vol_df = merged_df.copy()\n",
    "\n",
    "# Set the prediction lag in days\n",
    "prediction_days = 10  # Adjust this value as needed\n",
    "\n",
    "# Shift volume forward (future volume)\n",
    "polarity_vol_df['future_volume'] = polarity_vol_df['volume'].shift(-prediction_days)\n",
    "\n",
    "# Calculate smoothened versions\n",
    "window_size = 20\n",
    "polarity_vol_df['smooth_compound'] = polarity_vol_df['compound'].rolling(window=window_size).mean()\n",
    "polarity_vol_df['smooth_future_volume'] = polarity_vol_df['future_volume'].rolling(window=window_size).mean()\n",
    "\n",
    "# Remove NaN values created by the shift and smoothing\n",
    "polarity_vol_df = polarity_vol_df.dropna(subset=['smooth_compound', 'smooth_future_volume'])\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add smoothened traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=polarity_vol_df['date'], y=polarity_vol_df['smooth_compound'], \n",
    "               name=\"Current Sentiment\", line=dict(color='blue', width=2.5)),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=polarity_vol_df['date'], y=polarity_vol_df['smooth_future_volume'], \n",
    "               name=f\"Volume in {prediction_days} days\", line=dict(color='red', width=2.5)),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=f\"Current Sentiment vs {prediction_days}-day Future Volume for {ITEM}<br>({window_size}-day moving average)\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    margin=dict(l=50, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_yaxes(title_text=\"Sentiment Score (Compound)\", secondary_y=False, gridcolor='lightgrey')\n",
    "fig.update_yaxes(title_text=f\"Volume Traded in {prediction_days} days\", secondary_y=True, gridcolor='lightgrey')\n",
    "fig.update_xaxes(title_text=\"Date\", gridcolor='lightgrey')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Save figures\n",
    "save_dir = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'figures', ITEM_SANITIZED)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "fig.write_image(\n",
    "    os.path.join(save_dir, f'timeseries_pol_vol_prediction_{prediction_days}days.png'),\n",
    "    width=1920, height=1080, scale=2\n",
    ")\n",
    "\n",
    "fig.write_html(os.path.join(save_dir, f'timeseries_pol_vol_prediction_{prediction_days}days.html'))\n",
    "print(f\"Saved to ./data/figures/{ITEM_SANITIZED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timeseries - compound polarity against price plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for this analysis\n",
    "polarity_price_df = merged_df.copy()\n",
    "\n",
    "# Set the prediction lag in days\n",
    "prediction_days = 1  # Adjust this value as needed\n",
    "\n",
    "# Shift price forward (future price)\n",
    "polarity_price_df['future_price'] = polarity_price_df['price_usd'].shift(-prediction_days)\n",
    "\n",
    "# Calculate smoothened versions\n",
    "window_size = 10\n",
    "polarity_price_df['smooth_compound'] = polarity_price_df['compound'].rolling(window=window_size).mean()\n",
    "polarity_price_df['smooth_future_price'] = polarity_price_df['future_price'].rolling(window=window_size).mean()\n",
    "\n",
    "# Remove NaN values created by the shift and smoothing\n",
    "polarity_price_df = polarity_price_df.dropna(subset=['smooth_compound', 'smooth_future_price'])\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add smoothened traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=polarity_price_df['date'], y=polarity_price_df['smooth_compound'], \n",
    "               name=\"Current Sentiment\", line=dict(color='blue', width=2.5)),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=polarity_price_df['date'], y=polarity_price_df['smooth_future_price'], \n",
    "               name=f\"Price in {prediction_days} days\", line=dict(color='red', width=2.5)),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=f\"Current Sentiment vs {prediction_days}-day Future Price for {ITEM}<br>({window_size}-day moving average)\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "    margin=dict(l=50, r=50, t=80, b=50),\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_yaxes(title_text=\"Sentiment Score (Compound)\", secondary_y=False, gridcolor='lightgrey')\n",
    "fig.update_yaxes(title_text=f\"Price (USD) in {prediction_days} days\", secondary_y=True, gridcolor='lightgrey')\n",
    "fig.update_xaxes(title_text=\"Date\", gridcolor='lightgrey')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Save figures\n",
    "save_dir = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'figures', ITEM_SANITIZED)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "fig.write_image(\n",
    "    os.path.join(save_dir, f'timeseries_pol_price_prediction_{prediction_days}days.png'),\n",
    "    width=1920, height=1080, scale=2\n",
    ")\n",
    "\n",
    "fig.write_html(os.path.join(save_dir, f'timeseries_pol_price_prediction_{prediction_days}days.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowed scatterplot - compound polarity & volume plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your date range\n",
    "start_date = pd.to_datetime('2019-01-01')\n",
    "end_date = pd.to_datetime('2019-02-01')\n",
    "window = 20\n",
    "\n",
    "# Filter dataframe for date range and create a copy\n",
    "scatter_vol_df = merged_df[(merged_df['date'] >= start_date) & \n",
    "                          (merged_df['date'] <= end_date)].copy()\n",
    "\n",
    "# Calculate smoothed values\n",
    "scatter_vol_df['smoothed_compound'] = scatter_vol_df['compound'].rolling(window=window).mean()\n",
    "scatter_vol_df['smoothed_volume'] = scatter_vol_df['volume'].rolling(window=window).mean()\n",
    "\n",
    "# Remove any NaN values before fitting\n",
    "clean_df = scatter_vol_df.dropna(subset=['smoothed_volume', 'smoothed_compound'])\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter points\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=clean_df['smoothed_volume'],\n",
    "        y=clean_df['smoothed_compound'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8),\n",
    "        name='Data Points',\n",
    "        text=clean_df['date'],\n",
    "        hovertemplate='Smoothed Volume: %{x}<br>Smoothed Sentiment: %{y}<br>Date: %{text}<extra></extra>'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(clean_df['smoothed_volume'], clean_df['smoothed_compound'], 1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=clean_df['smoothed_volume'],\n",
    "        y=z[0] * clean_df['smoothed_volume'] + z[1],\n",
    "        mode='lines',\n",
    "        name=f'Trend line',\n",
    "        line=dict(color='red')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f\"Smoothed Sentiment vs. Smoothed Volume for {ITEM}<br>Date Range: {start_date.date()} to {end_date.date()}<br>Smoothing Window: {window} days\",\n",
    "    xaxis_title=\"Smoothed Volume Traded\",\n",
    "    yaxis_title=\"Smoothed Sentiment Score (Compound)\",\n",
    "    height=600,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Save figures\n",
    "save_dir = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'figures', ITEM_SANITIZED)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(os.path.join(save_dir, f'scatter_pol_vol_{i}.png')):\n",
    "    i += 1\n",
    "\n",
    "fig.write_image(\n",
    "    os.path.join(save_dir, f'scatter_pol_vol_{i}.png'),\n",
    "    width=1920, \n",
    "    height=1080,\n",
    "    scale=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowed scatterplot - compound polarity & volume plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your date range\n",
    "start_date = pd.to_datetime('2020-05-01')\n",
    "end_date = pd.to_datetime('2020-06-01')\n",
    "window = 10\n",
    "\n",
    "# Filter dataframe for date range and create a copy\n",
    "scatter_price_df = merged_df[(merged_df['date'] >= start_date) & \n",
    "                            (merged_df['date'] <= end_date)].copy()\n",
    "\n",
    "# Calculate smoothed values\n",
    "scatter_price_df['smoothed_compound'] = scatter_price_df['compound'].rolling(window=window).mean()\n",
    "scatter_price_df['smoothed_price'] = scatter_price_df['price_usd'].rolling(window=window).mean()\n",
    "\n",
    "# Remove any NaN values before fitting\n",
    "clean_df = scatter_price_df.dropna(subset=['smoothed_price', 'smoothed_compound'])\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter points\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=clean_df['smoothed_price'],\n",
    "        y=clean_df['smoothed_compound'],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8),\n",
    "        name='Data Points',\n",
    "        text=clean_df['date'],\n",
    "        hovertemplate='Smoothed Price: %{x}<br>Smoothed Sentiment: %{y}<br>Date: %{text}<extra></extra>'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add regression line if we have valid data\n",
    "if len(clean_df) > 1:  # Need at least 2 points for a line\n",
    "    z = np.polyfit(clean_df['smoothed_price'], clean_df['smoothed_compound'], 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=clean_df['smoothed_price'],\n",
    "            y=z[0] * clean_df['smoothed_price'] + z[1],\n",
    "            mode='lines',\n",
    "            name=f'Trend line',\n",
    "            line=dict(color='red')\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=f\"Smoothed Sentiment vs. Smoothed Price for {ITEM}<br>Date Range: {start_date.date()} to {end_date.date()}<br>Smoothing Window: {window} days\",\n",
    "    xaxis_title=\"Smoothed Price (USD)\",\n",
    "    yaxis_title=\"Smoothed Sentiment Score (Compound)\",\n",
    "    height=600,\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Save figures\n",
    "save_dir = os.path.join(GAME_PRICE_PREDICTION_PATH, 'data', 'figures', ITEM_SANITIZED)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(os.path.join(save_dir, f'scatter_pol_price_{i}.png')):\n",
    "    i += 1\n",
    "\n",
    "fig.write_image(\n",
    "    os.path.join(save_dir, f'scatter_pol_price_{i}.png'),\n",
    "    width=1920, \n",
    "    height=1080,\n",
    "    scale=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate stats for compound polarity against volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for statistical analysis\n",
    "stats_vol_df = merged_df.copy()\n",
    "\n",
    "# Calculate smoothened versions with 30-day window\n",
    "window_size = 30\n",
    "stats_vol_df['smooth_compound'] = stats_vol_df['compound'].rolling(window=window_size).mean()\n",
    "stats_vol_df['smooth_volume'] = stats_vol_df['volume'].rolling(window=window_size).mean()\n",
    "stats_vol_df = stats_vol_df.dropna()\n",
    "\n",
    "print(\"Item name: \" + ITEM)\n",
    "\n",
    "# 1. Test for normality first (this affects which correlation test to use)\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "print(\"\\nNormality Tests:\")\n",
    "_, p_value_compound = shapiro(stats_vol_df['smooth_compound'])\n",
    "print(f\"Compound Sentiment - Shapiro p-value: {p_value_compound}\")\n",
    "_, p_value_volume = shapiro(stats_vol_df['smooth_volume'])\n",
    "print(f\"Volume - Shapiro p-value: {p_value_volume}\")\n",
    "\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "if p_value_compound < 0.05 or p_value_volume < 0.05:\n",
    "    # If not normal, use Spearman\n",
    "    spearman_corr, spearman_p = spearmanr(stats_vol_df['smooth_compound'], stats_vol_df['smooth_volume'])\n",
    "    print(f\"Using Spearman (non-parametric) due to non-normal distribution\")\n",
    "    print(f\"Correlation: {spearman_corr:.3f}, p-value: {spearman_p:.3f}\")\n",
    "else:\n",
    "    # If normal, use Pearson\n",
    "    pearson_corr, pearson_p = pearsonr(stats_vol_df['smooth_compound'], stats_vol_df['smooth_volume'])\n",
    "    print(f\"Using Pearson (parametric) as data is normally distributed\")\n",
    "    print(f\"Correlation: {pearson_corr:.3f}, p-value: {pearson_p:.3f}\")\n",
    "\n",
    "# 3. Stationarity Test with interpretation\n",
    "adf_volume = adfuller(stats_vol_df['smooth_volume'])\n",
    "adf_compound = adfuller(stats_vol_df['smooth_compound'])\n",
    "\n",
    "print(\"\\nStationarity Tests:\")\n",
    "print(\"Volume:\")\n",
    "print(f\"ADF Statistic: {adf_volume[0]:.3f}\")\n",
    "print(f\"p-value: {adf_volume[1]:.3f}\")\n",
    "print(\"Stationary\" if adf_volume[1] < 0.05 else \"Non-stationary\")\n",
    "\n",
    "print(\"\\nCompound Sentiment:\")\n",
    "print(f\"ADF Statistic: {adf_compound[0]:.3f}\")\n",
    "print(f\"p-value: {adf_compound[1]:.3f}\")\n",
    "print(\"Stationary\" if adf_compound[1] < 0.05 else \"Non-stationary\")\n",
    "\n",
    "# 4. Effect Size (Cohen's d)\n",
    "from scipy import stats\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2\n",
    "    return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.std(x, ddof=1) ** 2 + (ny-1)*np.std(y, ddof=1) ** 2) / dof)\n",
    "\n",
    "print(\"\\nEffect Size:\")\n",
    "d = cohens_d(stats_vol_df['smooth_compound'], stats.zscore(stats_vol_df['smooth_volume']))\n",
    "print(f\"Cohen's d: {d:.3f}\")\n",
    "print(\"Effect size interpretation:\")\n",
    "print(\"Small\" if abs(d) < 0.5 else \"Medium\" if abs(d) < 0.8 else \"Large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate stats for compound polarity against price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for statistical analysis\n",
    "stats_price_df = merged_df.copy()\n",
    "\n",
    "# Calculate smoothened versions with 30-day window\n",
    "window_size = 30\n",
    "stats_price_df['smooth_compound'] = stats_price_df['compound'].rolling(window=window_size).mean()\n",
    "stats_price_df['smooth_price'] = stats_price_df['price_usd'].rolling(window=window_size).mean()\n",
    "stats_price_df = stats_price_df.dropna()\n",
    "\n",
    "print(\"Item name: \" + ITEM)\n",
    "\n",
    "# 1. Test for normality first (this affects which correlation test to use)\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "print(\"\\nNormality Tests:\")\n",
    "_, p_value_compound = shapiro(stats_price_df['smooth_compound'])\n",
    "print(f\"Compound Sentiment - Shapiro p-value: {p_value_compound}\")\n",
    "_, p_value_price = shapiro(stats_price_df['smooth_price'])\n",
    "print(f\"Price - Shapiro p-value: {p_value_price}\")\n",
    "\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "if p_value_compound < 0.05 or p_value_price < 0.05:\n",
    "    # If not normal, use Spearman\n",
    "    spearman_corr, spearman_p = spearmanr(stats_price_df['smooth_compound'], stats_price_df['smooth_price'])\n",
    "    print(f\"Using Spearman (non-parametric) due to non-normal distribution\")\n",
    "    print(f\"Correlation: {spearman_corr:.3f}, p-value: {spearman_p:.3f}\")\n",
    "else:\n",
    "    # If normal, use Pearson\n",
    "    pearson_corr, pearson_p = pearsonr(stats_price_df['smooth_compound'], stats_price_df['smooth_price'])\n",
    "    print(f\"Using Pearson (parametric) as data is normally distributed\")\n",
    "    print(f\"Correlation: {pearson_corr:.3f}, p-value: {pearson_p:.3f}\")\n",
    "\n",
    "# 3. Stationarity Test with interpretation\n",
    "adf_price = adfuller(stats_price_df['smooth_price'])\n",
    "adf_compound = adfuller(stats_price_df['smooth_compound'])\n",
    "\n",
    "print(\"\\nStationarity Tests:\")\n",
    "print(\"Price:\")\n",
    "print(f\"ADF Statistic: {adf_price[0]:.3f}\")\n",
    "print(f\"p-value: {adf_price[1]:.3f}\")\n",
    "print(\"Stationary\" if adf_price[1] < 0.05 else \"Non-stationary\")\n",
    "\n",
    "print(\"\\nCompound Sentiment:\")\n",
    "print(f\"ADF Statistic: {adf_compound[0]:.3f}\")\n",
    "print(f\"p-value: {adf_compound[1]:.3f}\")\n",
    "print(\"Stationary\" if adf_compound[1] < 0.05 else \"Non-stationary\")\n",
    "\n",
    "# 4. Effect Size (Cohen's d)\n",
    "from scipy import stats\n",
    "\n",
    "def cohens_d(x, y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2\n",
    "    return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.std(x, ddof=1) ** 2 + (ny-1)*np.std(y, ddof=1) ** 2) / dof)\n",
    "\n",
    "print(\"\\nEffect Size:\")\n",
    "d = cohens_d(stats_price_df['smooth_compound'], stats.zscore(stats_price_df['smooth_price']))\n",
    "print(f\"Cohen's d: {d:.3f}\")\n",
    "print(\"Effect size interpretation:\")\n",
    "print(\"Small\" if abs(d) < 0.5 else \"Medium\" if abs(d) < 0.8 else \"Large\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
