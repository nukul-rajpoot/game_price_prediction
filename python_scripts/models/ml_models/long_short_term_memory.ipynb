{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:54:02.713928Z","iopub.status.busy":"2024-08-27T11:54:02.713535Z","iopub.status.idle":"2024-08-27T11:54:02.735860Z","shell.execute_reply":"2024-08-27T11:54:02.734897Z","shell.execute_reply.started":"2024-08-27T11:54:02.713891Z"},"trusted":true},"outputs":[],"source":["import sys\n","import plotly.graph_objects as go\n","import pandas as pd\n","import requests\n","import numpy as np\n","import os\n","import random\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.callbacks import TensorBoard\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import explained_variance_score\n","from sklearn.metrics import median_absolute_error\n","from sklearn.preprocessing import StandardScaler\n","import skopt\n","from skopt import gp_minimize, forest_minimize\n","from skopt.space import Real, Categorical, Integer\n","from skopt.plots import plot_convergence\n","from skopt.plots import plot_objective, plot_evaluations\n","from skopt.plots import plot_histogram, plot_objective_2D\n","from skopt.utils import use_named_args\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import LayerNormalization\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","from kerastuner import RandomSearch, Hyperband, BayesianOptimization\n","sys.path.insert(0, os.path.abspath('../..'))\n","\n","    \n","\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Get original DataFrame"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#DO NOT USE FOR LOCAL - KAGGLE ONLY\n","def fetch_item_from_api(item, dailyCookie):\n","    # get historical price data of item from API\n","    url = \"https://steamcommunity.com/market/pricehistory/\"\n","    params = {\n","        'country': 'US',\n","        'currency': '1',\n","        'appid': '730',\n","        'market_hash_name': item\n","    }\n","    cookies = {'steamLoginSecure': dailyCookie}\n","\n","    response = requests.get(url, params=params, cookies=cookies)\n","    json_data = response.json()\n","    \n","    # print error message if request failed\n","    if response.status_code != 200:\n","        print(f\"Failed to fetch data for {item}. Status code: {response.status_code}\")\n","        return None \n","           \n","    # convert and clean data to dataframe object\n","    price_history = json_data['prices']\n","    price_history_df = pd.DataFrame(price_history, columns=['date', 'price_usd', 'volume'])\n","    price_history_df['date'] = pd.to_datetime(price_history_df['date'].str[0:-4], format='%b %d %Y %H')\n","    price_history_df['volume'] = pd.to_numeric(price_history_df['volume'])\n","    price_history_df.set_index('date', inplace=True)\n","   \n","    return price_history_df\n","def fetch_item_to_df(item, dailyCookie):\n","    price_history_df = fetch_item_from_api(item, dailyCookie)\n","    grouped_current_item = price_history_df.groupby(pd.Grouper(freq='D')).agg({\n","    'price_usd':'median',\n","    'volume':'sum'\n","    })\n","    return grouped_current_item\n","\n","def get_cookie_from_blob():\n","    blob_url = \"https://steamgraphsstorage.blob.core.windows.net/container-for-blob/cookie.txt?sp=rwd&st=2024-08-06T20:45:18Z&se=2025-09-10T04:45:18Z&spr=https&sv=2022-11-02&sr=c&sig=MKticGz9P9HPI7iXp1a6yuErc5Sv6P9fY%2FfCbxL0PLg%3D\"\n","    response = requests.get(blob_url)\n","    response.raise_for_status()\n","    return response.text\n","\n","\n","def fetch_items():\n","    items = [\"Glove Case Key\", \"Officer Jacques Beltram | Gendarmerie Nationale\", \"Kilowatt Case\", \"AK-47 | Blue Laminate (Factory New)\", \"Glove Case\", \"★ StatTrak™ Paracord Knife | Case Hardened (Field-Tested)\"]\n","    return items\n","\n","dailyCookie = get_cookie_from_blob()\n","items = fetch_items()\n","current_item = fetch_item_to_df(items[4], dailyCookie)\n","df = current_item\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:54:04.153304Z","iopub.status.busy":"2024-08-27T11:54:04.152952Z","iopub.status.idle":"2024-08-27T11:54:05.734180Z","shell.execute_reply":"2024-08-27T11:54:05.733247Z","shell.execute_reply.started":"2024-08-27T11:54:04.153268Z"},"trusted":true},"outputs":[],"source":["\n","\n","dailyCookie = get_cookie_from_blob()\n","items = fetch_items()\n","\n","current_item = fetch_item_to_df(items[4], dailyCookie)\n","#print(items[4])\n","\n","#print(current_item.tail())\n","#print(non_aggregated_item.tail())\n","\n"," #weekly aggregated data\n","current_item_weekly= current_item.groupby(pd.Grouper(freq='W')).agg({\n","    'price_usd':'median',\n","    })\n","df = current_item_weekly\n","print(df)\n","os. getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:54:05.736508Z","iopub.status.busy":"2024-08-27T11:54:05.736199Z","iopub.status.idle":"2024-08-27T11:54:05.745590Z","shell.execute_reply":"2024-08-27T11:54:05.744547Z","shell.execute_reply.started":"2024-08-27T11:54:05.736476Z"},"trusted":true},"outputs":[],"source":["df['price_delta'] = df['price_usd'].diff()\n","print(df)\n","\n","\n","# Fill missing values if any\n","df = df.fillna(method='ffill')\n","\n","# Scale the data\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","df['scaled_price'] = scaler.fit_transform(df[['price_usd']])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:54:05.747012Z","iopub.status.busy":"2024-08-27T11:54:05.746723Z","iopub.status.idle":"2024-08-27T11:54:05.764321Z","shell.execute_reply":"2024-08-27T11:54:05.763360Z","shell.execute_reply.started":"2024-08-27T11:54:05.746980Z"},"trusted":true},"outputs":[],"source":["seq_length = 2  # Length of the sequence\n","y_shift = -1 # t+x should be a negative value for predictions future\n","\n","counter = 0\n","# Create columns for each step in the sequence\n","for i in range(1, seq_length + 1):\n","    df[f't-{i}'] = df['scaled_price'].shift(i)\n","    counter += 1\n","\n","\n","df['y_shift'] = df['scaled_price'].shift(y_shift)  # y is the next value in the sequence (t+1)\n","# Drop rows with NaN values that were introduced by shifting\n","\n","df.dropna(inplace=True)\n","print(df)\n","print(counter)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:54:05.766514Z","iopub.status.busy":"2024-08-27T11:54:05.765860Z","iopub.status.idle":"2024-08-27T11:54:05.780334Z","shell.execute_reply":"2024-08-27T11:54:05.778981Z","shell.execute_reply.started":"2024-08-27T11:54:05.766468Z"},"trusted":true},"outputs":[],"source":["X_labels = [f't-{i}' for i in range(1, seq_length + 1)]\n","y_label = 'y_shift' #(t+1)\n","\n","X = df[X_labels]\n","y = df[y_label]\n","\n","split_train = int(0.6 * len(X))  # First 60% for training\n","split_val = int(0.75 * len(X))   #next 15 for val, then rest for backtesting\n","\n","# Split the data\n","X_train = X[:split_train]\n","y_train = y[:split_train]\n","\n","X_val = X[split_train:split_val]\n","y_val = y[split_train:split_val]\n","\n","\n","#convert to numpy arrays\n","X_valnp = X_val.to_numpy()\n","y_valnp = y_val.to_numpy()\n","#X_valnp_reshape = np.reshape(X_valnp, (X_valnp.shape[0], 1, X_valnp.shape[1]))\n","validation_data = (X_valnp, y_valnp)\n","\n","#print((X_valnp))\n","\n","X_test = X[split_val:]\n","y_test = y[split_val:]\n","\n","\n","\n","# scaler = StandardScaler()\n","# X_train_scaled = scaler.fit_transform(X_train)\n","# X_test_scaled = scaler.transform(X_test)\n","#X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n","#X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n","\n","X_test_reshaped = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n","#test_loss = best_model.evaluate(X_test_reshaped, y_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["Creates and fit the LSTM network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:54:05.782187Z","iopub.status.busy":"2024-08-27T11:54:05.781871Z","iopub.status.idle":"2024-08-27T11:54:05.794978Z","shell.execute_reply":"2024-08-27T11:54:05.794042Z","shell.execute_reply.started":"2024-08-27T11:54:05.782125Z"},"trusted":true},"outputs":[],"source":["\n","def build_model(hp):\n","    model = keras.Sequential()\n","    \n","    #Tune the batch size\n","    hp.Choice('batch_size',values = [32,64,128,256])\n","    # Tune the number of LSTM layers and units\n","    for i in range(hp.Int('num_lstm_layers', 1, 3)):\n","        model.add(layers.LSTM(\n","            units=hp.Int(f'lstm_units_{i}', min_value=32, max_value=512, step=32),\n","            return_sequences=True if i < hp.Int('num_lstm_layers', 1, 3) - 1 else False,\n","            input_shape=(seq_length, 1) if i == 0 else None\n","        ))\n","    \n","    # Tune the number of dense layers and units\n","    for i in range(hp.Int('num_dense_layers', 1, 3)):\n","        model.add(layers.Dense(\n","            units=hp.Int(f'dense_units_{i}', min_value=32, max_value=512, step=32),\n","            activation=hp.Choice('dense_activation', ['relu', 'tanh'])\n","        ))\n","    \n","    # Output layer\n","    model.add(layers.Dense(1))\n","    \n","    # Tune learning rate\n","    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n","    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","    \n","    model.compile(optimizer=optimizer, loss='mean_squared_error')###mse usually\n","    return model\n","\n","# Instantiate the tuner\n","tuner = BayesianOptimization(\n","    build_model,\n","    objective='val_loss',\n","    max_trials=2,\n","    directory='keras_tuner_dir',\n","    project_name='lstm_tuning'\n",")\n","\n","# Reshape input data for LSTM\n","X_train_reshaped = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_val_reshaped = X_val.values.reshape((X_val.shape[0], X_val.shape[1], 1))\n","\n","# Perform the search\n","tuner.search(X_train_reshaped, y_train, \n","             epochs=10, \n","             validation_data=(X_val_reshaped, y_val),\n","             callbacks=[keras.callbacks.EarlyStopping(patience=5)],\n","            batch_size=64)\n","\n","# Get the best model\n","best_model = tuner.get_best_models(num_models=1)[0]\n","\n","# Print the best hyperparameters\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","print(\"Best Hyperparameters:\")\n","for hp in best_hps.values:\n","    print(f\"{hp}: {best_hps.get(hp)}\")\n","\n","# Evaluate the model\n","test_loss = best_model.evaluate(X_test_reshaped, y_test)\n","print(f\"Test Loss: {test_loss}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Hyperparameter Tuning"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:54:05.796619Z","iopub.status.busy":"2024-08-27T11:54:05.796339Z","iopub.status.idle":"2024-08-27T11:54:05.822081Z","shell.execute_reply":"2024-08-27T11:54:05.821169Z","shell.execute_reply.started":"2024-08-27T11:54:05.796588Z"},"trusted":true},"outputs":[],"source":["#COMMENTED AS THIS MAY COME IN HANDY IN FUTURE\n","\n","# def log_dir_name(learning_rate, num_dense_layers,\n","#                  num_dense_nodes, activation,num_lstm_layers,num_lstm_nodes):\n","\n","#     # The dir-name for the TensorBoard log-dir.\n","#     s = \"./19_logs/lr_{0:.0e}_layers_{1}_nodes_{2}_{3}/\"\n","\n","#     # Insert all the hyper-parameters in the dir-name.\n","#     log_dir = s.format(learning_rate,\n","#                        num_dense_layers,\n","#                        num_dense_nodes,\n","#                        activation,num_lstm_layers,num_lstm_nodes)\n","\n","#     return log_dir\n","\n","# dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n","#                          name='learning_rate')\n","# dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n","# dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n","# dim_activation = Categorical(categories=['relu', 'sigmoid'],\n","#                              name='activation')\n","# dim_num_lstm_layers = Integer(low=0, high =3, name = 'num_lstm_layers')\n","# dim_num_lstm_nodes=Integer(low=4, high=32,name = 'num_lstm_nodes')\n","# print(type(dim_num_lstm_nodes))\n","# #dim_num_lstm_nodes = Integer(low=1, high=5, name='num_lstm_nodes')\n","# dimensions = [dim_learning_rate,\n","#               dim_num_dense_layers,\n","#               dim_num_dense_nodes,\n","#               dim_activation,dim_num_lstm_layers, dim_num_lstm_nodes]\n","# default_parameters = [1e-5, 1, 16, 'relu',1, 8]\n","# print(type(dim_activation))\n","# print(type(dim_learning_rate))\n","# print(type(dim_num_dense_layers))\n","# print(type(dim_num_dense_nodes))\n","# #def create_model():\n","#   #  model = Sequential()\n","#    # model.add(LSTM(4, input_shape=(seq_length, 1)))\n","#    # model.add(Dense(1))\n","#   #  model.compile(loss='mean_squared_error', optimizer='adam')\n","#    # model.fit(X_train, y_train, epochs=2, batch_size=1, verbose=2)\n","\n","#     #return model\n","\n","\n","\n","# best_mse = 100000\n","# @use_named_args(dimensions=dimensions)\n","# def fitness(learning_rate, num_dense_layers,\n","#             num_dense_nodes, activation,num_lstm_layers, num_lstm_nodes):\n","#     \"\"\"\n","#     Hyper-parameters:\n","#     learning_rate:     Learning-rate for the optimizer.\n","#     num_dense_layers:  Number of dense layers.\n","#     num_dense_nodes:   Number of nodes in each dense layer.\n","#     activation:        Activation function for all layers.\n","#     \"\"\"\n","\n","#     # Print the hyper-parameters.\n","#     print('learning rate: {0:.1e}'.format(learning_rate))\n","#     print('num_dense_layers:', num_dense_layers)\n","#     print('num_dense_nodes:', num_dense_nodes)\n","#     print('activation:', activation)\n","#     print('num_lstm_layers:', num_lstm_layers)\n","#     print('num_lstm_nodes:', num_lstm_nodes,type(num_lstm_nodes))\n","    \n","    \n","#     # Create the neural network with these hyper-parameters.\n","#     model = create_model(learning_rate=learning_rate,\n","#                          num_dense_layers=num_dense_layers,\n","#                          num_dense_nodes=num_dense_nodes,\n","#                          activation=activation,num_lstm_layers=num_lstm_layers,num_lstm_nodes=num_lstm_nodes)\n","\n","#     # Dir-name for the TensorBoard log-files.\n","#     log_dir = log_dir_name(learning_rate, num_dense_layers,\n","#                            num_dense_nodes, activation,num_lstm_layers,num_lstm_nodes)\n","    \n","#     # Create a callback-function for Keras which will be\n","#     # run after each epoch has ended during training.\n","#     # This saves the log-files for TensorBoard.\n","#     # Note that there are complications when histogram_freq=1.\n","#     # It might give strange errors and it also does not properly\n","#     # support Keras data-generators for the validation-set.\n","#     callback_log = TensorBoard(\n","#         log_dir=log_dir,\n","#         histogram_freq=0,\n","#         write_graph=True,\n","#         # write_grads=False,\n","#         write_images=False)\n","   \n","\n","\n","#     earlyStop=EarlyStopping(monitor=\"val_loss\",verbose=2,mode='min',patience=3)\n","  \n","    \n","#     # Use Keras to train the model.\n","#     history = model.fit(x=X_train,\n","#                         y=y_train,\n","#                         epochs=40,\n","#                         batch_size=64,\n","#                         validation_data=validation_data,\n","#                         callbacks=[earlyStop])\n","\n","#     # Get the classification accuracy on the validation-set\n","#     # after the last training-epoch.\n","#     mse = history.history['mean_squared_error'][-1]\n","\n","#     # Print the classification accuracy.\n","#     print()\n","#     print(\"Mean Squared Error:\",mse)\n","#     print()\n","\n","#     # Save the model if it improves on the best-found performance.\n","#     # We use the global keyword so we update the variable outside\n","#     # of this function.\n","#     global best_mse\n","#     #If the classification accuracy of the saved model is improved ...\n","#     if mse < best_mse:\n","#         # Save the new model to harddisk.\n","#         model.save('best_lstm_model.keras')\n","#         #model is saved in same location as notebook\n","#         # Update the classification accuracy.\n","#         best_mse = mse\n","\n","#     # Delete the Keras model with these hyper-parameters from memory.\n","#     del model\n","    \n","#     # Clear the Keras session, otherwise it will keep adding new\n","#     # models to the same TensorFlow graph each time we create\n","#     # a model with a different set of hyper-parameters.\n","#     K.clear_session()\n","    \n","#     # NOTE: Scikit-optimize does minimization so it tries to\n","#     # find a set of hyper-parameters with the LOWEST fitness-value.\n","#     # Because we are interested in the HIGHEST classification\n","#     # accuracy, we need to negate this number so it can be minimized.\n","#     return mse\n","\n","\n","\n","# fitness(x=default_parameters)\n","# search_result = gp_minimize(func=fitness,\n","#                             dimensions=dimensions,\n","#                             acq_func='EI', # Expected Improvement.\n","#                             n_calls=40,\n","#                             x0=default_parameters)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:59:14.867721Z","iopub.status.busy":"2024-08-27T11:59:14.866748Z","iopub.status.idle":"2024-08-27T11:59:15.564041Z","shell.execute_reply":"2024-08-27T11:59:15.563290Z","shell.execute_reply.started":"2024-08-27T11:59:14.867648Z"},"trusted":true},"outputs":[],"source":["#best_model=tf.keras.models.load_model('best_lstm_model.keras')\n","#model is saved in same location as notebook\n","#best_model = create_old_model()\n","# Convert delta predictions to absolute prices\n","\n","\n","# Get actual prices for comparison\n","actual_prices = df['price_usd'].iloc[split_train:split_val].values\n","y_valpred = best_model.predict(X_val)\n","last_known_price = df['price_usd'].iloc[split_train - 1]  # Last known price before validation set\n","predicted_prices = [last_known_price]\n","for delta in y_valpred:\n","    predicted_prices.append(predicted_prices[-1] + scaler.inverse_transform(delta.reshape(1, -1))[0][0])\n","\n","predicted_prices = predicted_prices[1:]  # Remove the initial price\n","unscaledvalpred = scaler.inverse_transform(y_valpred)\n","unscaledvaly = scaler.inverse_transform([y_val])\n","rowunscaledvaly = unscaledvaly.reshape(-1,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:59:15.565561Z","iopub.status.busy":"2024-08-27T11:59:15.565249Z","iopub.status.idle":"2024-08-27T11:59:15.577362Z","shell.execute_reply":"2024-08-27T11:59:15.576448Z","shell.execute_reply.started":"2024-08-27T11:59:15.565527Z"},"trusted":true},"outputs":[],"source":["testScore = np.sqrt(mean_squared_error(unscaledvaly[0], unscaledvalpred[:,0]))  #Root mean squared error\n","r2 = r2_score(unscaledvaly[0], unscaledvalpred[:, 0]) #R2 score - This provides an indication of the goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model. It is the proportion of the variance in the dependent variable that is predictable from the independent variables.\n","explained_variance = explained_variance_score(unscaledvaly[0], unscaledvalpred[:, 0])  #Explained variance score  this measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set. It is the proportion of the variance in the dependent variable that is predictable from the independent variables\n","medae = median_absolute_error(unscaledvaly[0], unscaledvalpred[:, 0])\n","mae = mean_absolute_error(unscaledvaly[0], unscaledvalpred[:, 0])\n","mape = np.mean(np.abs((unscaledvaly[0] - unscaledvalpred[:, 0]) / unscaledvaly[0])) * 100\n","\n","print('Test Score: %.2f RMSE' % (testScore))\n","print('R2 Score: %.2f' % (r2))\n","print('Explained Variance: %.2f' % (explained_variance))\n","print('Median Absolute Error: %.2f' % (medae))\n","print('Mean Absolute Error: %.2f' % (mae))\n","print('Mean Absolute Percentage Error: %.2f' % (mape)+'%')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:59:15.578756Z","iopub.status.busy":"2024-08-27T11:59:15.578465Z","iopub.status.idle":"2024-08-27T11:59:15.873552Z","shell.execute_reply":"2024-08-27T11:59:15.872621Z","shell.execute_reply.started":"2024-08-27T11:59:15.578725Z"},"trusted":true},"outputs":[],"source":["# # shift train predictions for plotting\n","# trainPredictPlot = np.empty_like(dataset)\n","\n","# trainPredictPlot[:, :] = np.nan\n","\n","# #trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n","\n","\n","\n","plt.grid()\n","plt.title(\"LSTM - Predicted vs Actual\")\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Price (USD)\")\n","plt.plot(unscaledvalpred , label = \"Predicted\") #ȳ\n","plt.plot(rowunscaledvaly, label = \"Actual\") #ground truth values\n","plt.legend(loc=\"upper left\")\n","plt.show()\n","print(testScore)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-27T11:59:15.875103Z","iopub.status.busy":"2024-08-27T11:59:15.874798Z","iopub.status.idle":"2024-08-27T11:59:16.197455Z","shell.execute_reply":"2024-08-27T11:59:16.196537Z","shell.execute_reply.started":"2024-08-27T11:59:15.875071Z"},"trusted":true},"outputs":[],"source":["# plot_convergence(search_result)\n","# search_result.x\n","# space = search_result.space\n","\n","# search_result.fun\n","# sorted(zip(search_result.func_vals, search_result.x_iters)) #MSE\n","# print()\n","# sorted(zip(search_result.func_vals, search_result.x_iters))\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
